{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "from bert_score import score as score1\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "\n",
    "api_key = \"your openai api key\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "#big 5 analysis\n",
    "with open(\"./withdbig5.json\", 'r', encoding='utf-8') as file:\n",
    "    big5 = json.load(file)\n",
    "\n",
    "#The work title corresponding to the character name\n",
    "with open('profiles-eng_scripts.json', 'r', encoding='utf-8') as file:\n",
    "    work = json.load(file)\n",
    "\n",
    "#character description\n",
    "with open('profiles-eng_desc.json', 'r', encoding='utf-8') as file:\n",
    "    desp = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate bert score\n",
    "def calculate_scores(df):\n",
    "    bert_scores_ans = []\n",
    "    no_tune_scores = []\n",
    "    tuned_simple_scores = []\n",
    "    simple_prompt_scores = []\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        # get reference txt and generated txt\n",
    "        reference = str(row['reference answer'])\n",
    "        answer = str(row['answer']) \n",
    "        notune_answer = str(row['answer without tune'])\n",
    "        tuned_simple = str(row['tuned simple prompt'])\n",
    "        simplep_answer = str(row['simple prompt answer'])\n",
    "\n",
    "        #calculate BERTScore \n",
    "        _, _, bert_score = score1([answer], [reference], lang=\"eng\", verbose=False)\n",
    "        bert_scores_ans.append(bert_score.mean().item())\n",
    "\n",
    "        _, _, bert_score1 = score1([notune_answer], [reference], lang=\"eng\", verbose=False)\n",
    "        no_tune_scores.append(bert_score1.mean().item())\n",
    "\n",
    "        _, _, bert_score1 = score1([tuned_simple], [reference], lang=\"eng\", verbose=False)\n",
    "        tuned_simple_scores.append(bert_score1.mean().item())\n",
    "\n",
    "        _, _, bert_score2 = score1([simplep_answer], [reference], lang=\"eng\", verbose=False)\n",
    "        simple_prompt_scores.append(bert_score2.mean().item())\n",
    "\n",
    "\n",
    "    # add scores into DataFrame\n",
    "    df['tuned score'] = bert_scores_ans\n",
    "    df['no tune score'] = no_tune_scores\n",
    "    df['tuned simple score'] = tuned_simple_scores\n",
    "    df['simple prompt score'] = simple_prompt_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "#calculate avg score of bert scores\n",
    "def get_mean(df):\n",
    "    avg_score = df['tuned score'].mean()\n",
    "    avg_score2 = df['no tune score'].mean()\n",
    "    avg_score3 = df['tuned simple score'].mean()\n",
    "    avg_score4 = df['simple prompt score'].mean()\n",
    "    print([avg_score,avg_score2,avg_score3,avg_score4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_name = \"HAL 9000\"#\"Willie Soke\" #\"Gaston\" #\"Naruto Uzumaki\"#\"Sasuke Uchiha\"\n",
    "path = f\"./result/{c_name}_120.csv\"\n",
    "#read generated results\n",
    "df = pd.read_csv(path)\n",
    "#calculate bert score\n",
    "df = calculate_scores(df)\n",
    "df.head()\n",
    "#save score\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and print average scores\n",
    "get_mean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT evaluation\n",
    "#prompt of winning rate evaluation\n",
    "def generate_prompt(df, role_name, big5):\n",
    "\n",
    "    path1 = f\"./profile/{role_name}.json\"\n",
    "    path2 = f\"./score/{role_name}.json\"\n",
    "\n",
    "    with open(path1, 'r', encoding='utf-8') as file:\n",
    "        profile = json.load(file)\n",
    "    \n",
    "    with open(path2, 'r', encoding='utf-8') as file:\n",
    "        score = json.load(file)\n",
    "\n",
    "    experience = profile[role_name]\n",
    "    description = desp[role_name]\n",
    "    big_5 = big5[role_name]\n",
    "\n",
    "    prompts = []\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        tuned_gpt_answer = row[\"Tuned GPT answer\"]\n",
    "        reference_answer = row['reference answer']\n",
    "        answer = row['answer']\n",
    "        no_tune_answer = row['answer without tune']\n",
    "        simple_prompt_answer = row['simple prompt answer']\n",
    "        tuned_simple_prompt = row['tuned simple prompt']\n",
    "\n",
    "        question_dict = {'question': question}\n",
    "        list_model_answer_dict = [\n",
    "            {'model': 'Tuned GPT3.5', 'answer': tuned_gpt_answer},\n",
    "            {'model': 'GPT3.5', 'answer': reference_answer},\n",
    "            {'model': 'Tuned Phi-3', 'answer': answer},\n",
    "            {'model': 'Tuned Phi-3 with simple prompt', 'answer': tuned_simple_prompt},\n",
    "            {'model': 'Phi-3', 'answer': no_tune_answer},\n",
    "            {'model': 'Phi-3 with simple prompt', 'answer': simple_prompt_answer}\n",
    "        ]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "System Instruction:\n",
    "You are a role-playing performance comparison assistant. You should rank the models based on the role characteristics and text quality of their responses. The rankings are then output using Python dictionaries and lists.\n",
    "User Prompt:\n",
    "The models below are to play the role of \"{role_name}\". The brief description of {role_name} is: {description}\n",
    "The Big-5 personality trait analysis of {role_name} is:{big_5}\n",
    "The relationships between {role_name} and some characters (0 means no, 1 means yes) and your familiarity and affection scores for them (from 1 to 10, the higher the score, the more familiar you are with them and the more you like them) are: \n",
    "{score}.\n",
    "I need to rank the following models based on the criteria below:\n",
    "1. Which one that better reflects the character's correct interpersonal relationships is better\n",
    "2. Which one has more pronounced role speaking style, and speaks more in line with the role description. The more distinctive the speaking style, the better.\n",
    "3. Which one's output can reflects the personality of the charactor more, is better.\n",
    "4. Which one's output contains more knowledge and memories related to the role; the richer, the better. (If the question contains reference answers, then the role-specific knowledge and memories are based on the reference answer.)\n",
    "5. Consider the character's limited knowledge base. If you think the question falls outside the character's knowledge range according to your judgment, then answers with an attitude of \"not knowing\" or \"not understanding\" will be better.\n",
    "The question provided to each model is:\n",
    "{question_dict}\n",
    "The respective answers from the models to this question are:\n",
    "{list_model_answer_dict}\n",
    "Now, based on the above two criteria, please rank the models. Avoid any positional biases and ensure that the order in which the responses are presented does not influence your decision. Do not favor certain model names.\n",
    "Then, use a list containing the model's name, its rank, and the reason for its ranking to return the results, i.e., please ensure to use the following format to return the results:\n",
    "[{{\"model\": <model-name>, \"reason\": <rank-reason>, \"rank\": <model-rank>}}, {{\"model\": <model-name>, \"reason\": <rank-reason>, \"rank'': <model-rank>}}]\n",
    "Your answer must be a valid Python list of dictionaries to ensure I can directly parse it using Python. Scrictly follow the format above. Do not include any extraneous content! Please provide a ranking that is as accurate as possible and aligns with the intuition of most people.\n",
    "        \"\"\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "#prompt of multi-dimension evaluation\n",
    "def generate_dimension_prompt(df, role_name, big5):\n",
    "\n",
    "    path1 = f\"./profile/{role_name}.json\"\n",
    "    path2 = f\"./score/{role_name}.json\"\n",
    "\n",
    "    with open(path1, 'r', encoding='utf-8') as file:\n",
    "        profile = json.load(file)\n",
    "    \n",
    "    with open(path2, 'r', encoding='utf-8') as file:\n",
    "        score = json.load(file)\n",
    "\n",
    "    description = desp[role_name]\n",
    "    big_5 = big5[role_name]\n",
    "\n",
    "    prompts = []\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        tuned_gpt_answer = row[\"Tuned GPT answer\"]\n",
    "        reference_answer = row['reference answer']\n",
    "        answer = row['answer']\n",
    "        no_tune_answer = row['answer without tune']\n",
    "        simple_prompt_answer = row['simple prompt answer']\n",
    "        tuned_simple_prompt = row['tuned simple prompt']\n",
    "\n",
    "        question_dict = {'question': question}\n",
    "        list_model_answer_dict = [\n",
    "            {'model': 'Tuned GPT3.5', 'answer': tuned_gpt_answer},\n",
    "            {'model': 'GPT3.5', 'answer': reference_answer},\n",
    "            {'model': 'Tuned Phi-3', 'answer': answer},\n",
    "            {'model': 'Tuned Phi-3 with simple prompt', 'answer': tuned_simple_prompt},\n",
    "            {'model': 'Phi-3', 'answer': no_tune_answer},\n",
    "            {'model': 'Phi-3 with simple prompt', 'answer': simple_prompt_answer}\n",
    "        ]\n",
    "        prompt = f\"\"\"\n",
    "System Instruction:\n",
    "You are a role-playing performance comparison assistant. You should rank the models based on the role characteristics and text quality of their responses. The rankings are then output using Python dictionaries and lists.\n",
    "User Prompt:\n",
    "The models below are to play the role of \"{role_name}\". The brief description of {role_name} is: {description}\n",
    "The Big-5 personality trait analysis of {role_name} is:{big_5}\n",
    "The relationships between {role_name} and some characters (0 means no, 1 means yes) and your familiarity and affection scores for them (from 1 to 10, the higher the score, the more familiar you are with them and the more you like them) are: \n",
    "{score}.\n",
    "I need to rank the following models based on the two criteria below:\n",
    "[Evaluation Criterion]\n",
    "Factual Correctness (1-7): Is the response provides truthful and detailed facts about the character?\n",
    "Personality (1-7): Is the response reflects the personalities and preferences of the character?\n",
    "Values (1-7): Is the response reflects the values and convictions of the character?\n",
    "Avoiding Hallucination (1-7): Is the response avoids to say things that the character do not know?\n",
    "[Evaluation Steps]\n",
    "[Factual Correctness]\n",
    "1. Read through the interactions and identify the key points related to the character.\n",
    "2. Read through the responses of the AI assistant and compare them to the profile. Check if the responses are consistent with the character's profile, background, and known facts about the character.\n",
    "3. Check whether the responses provide detailed facts about the character or if they are generic responses that could apply to any character. Detailed responses are more factual and contribute positively to the score.\n",
    "4. Rate the performance of the AI on a scale of 1-7 for factual correctness, where 1 is the lowest and 7 is the highest based on the Evaluation Criteria.\n",
    "[Personality]\n",
    "1. Read through the profile and write the personalities and preferences of the real character.\n",
    "2. Read through the interactions and identify the personalities and preferences of the AI assistant.\n",
    "3. After having a clear understanding of the interactions, compare the responses to the profile. Look for any consistencies or inconsistencies. Do the responses reflect the character's personalities and preferences?\n",
    "4. Use the given scale from 1-7 to rate how well the response reflects the personalities and preferences of the character. 1 being not at all reflective of the character's personalities, and 7 being perfectly reflective of the character's personalities.\n",
    "[Values]\n",
    "1. Read through the profile and write the values and convictions of the real character.\n",
    "2. Read through the interactions and identify the values and convictions of the AI assistant.\n",
    "3. After having a clear understanding of the interactions, compare the responses to the profile. Look for any consistencies or inconsistencies. Do the responses reflect the character's values and convictions?\n",
    "4. Use the given scale from 1-7 to rate how well the response reflects the values and convictions of the character. 1 being not at all reflective of the character's values, and 7 being perfectly reflective of the character's values.\n",
    "[Avoiding Hallucination]\n",
    "1. Read through the interactions and identify the knowledge scope of the character.\n",
    "2. Read through the responses of the AI assistant, find the evidence of knowledge used in the response.\n",
    "3. Compare the evidence to the profile. Check if the responses are consistent with the character's knowledge scope. If some knowledge contradicts to the character's identity, given a lower score. Otherwise, assign a higher score.\n",
    "4. Rate the performance of the AI on a scale of 1-7 for Avoiding Hallucination, where 1 is the lowest and 7 is the highest based on the Evaluation Criteria.\n",
    "The question provided to each model is:\n",
    "{question_dict}\n",
    "The respective answers from the models to this question are:\n",
    "{list_model_answer_dict}\n",
    "Now, based on the above criteria, please rank the models. Avoid any positional biases and ensure that the order in which the responses are presented does not influence your decision. Do not favor certain model names.\n",
    "Then, use a list containing the model's name, its score, and the reason for its score to return the results, i.e., please ensure to use the following format to return the results:\n",
    "[{{\"model\": <model-name>, \"reason\": <score-reason>, \"factual correctness score\": <model-score>,\"personality score\": <model-score>,\"values score\": <model-score>,\"avoiding hallucination score\": <model-score>}}, {{\"model\": <model-name>, \"reason\": <score-reason>, \"factual correctness score\": <model-score>,\"personality score\": <model-score>,\"values score\": <model-score>,\"avoiding hallucination score\": <model-score>}}]\n",
    "Your answer must be a valid Python list of dictionaries to ensure I can directly parse it using Python. Strictly follow the format above. Do not include any extraneous content! Please provide a ranking that is as accurate as possible and aligns with the intuition of most people.\n",
    "        \"\"\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "#prompt of big5 questionnaire evaluation\n",
    "def big5_eval(c_name,ver):\n",
    "    path = f\"./result/{c_name}_big5.json\"\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    with open(\"./big5_dim.json\", 'r', encoding='utf-8') as file:\n",
    "        intro = json.load(file)\n",
    "    \n",
    "    prompts = []\n",
    "    for item in data:\n",
    "        dim = item[\"dimension\"]\n",
    "        question = item[\"question\"]\n",
    "        if ver == 1:\n",
    "            answer = item[\"answer\"]\n",
    "        elif ver == 0:\n",
    "            answer = item[\"answer without tune\"]\n",
    "        elif ver == 2:\n",
    "            answer = item[\"gpt answer\"]\n",
    "            \n",
    "        dim_desp = intro[dim]\n",
    "        prompt = f\"\"\"\n",
    "You are a psychologist with expertise in personality theories. I'm conducting an experiment to evaluate participants' scores in the Big Five personality traits, especially on the {dim} dimension. For clarity, here's some background on differentiating this particular dimension and its factors:\n",
    "===\n",
    "{dim_desp}\n",
    "===\n",
    "\n",
    "I've invited a participant, {c_name}, and had the following conversations:\n",
    "===\n",
    "question:{question}\n",
    "answer:{answer}\n",
    "===\n",
    "\n",
    "Please help me evaluates whether {c_name} possesses a high {dim} personality or a low {dim} personality, and provide an integer score ranging from -5 to 5. \n",
    "\n",
    "Below are some scoring references. If the subject demonstrates a high {dim} personality in many factors, the score is 5 points. If the subject exhibits a high {dim} personality in a single factor, the score is 2 points. If the subject's personality cannot be determined, the score is 0 points. If the subject shows a low {dim} personality in one factor, the score is -2 points. If the subject indicates a low {dim} personality across multiple factors, the score is -5 points. \n",
    "\n",
    "Please output in the following json format:\n",
    "===\n",
    "{{\n",
    "    \"dimention\": \"{dim}\"\n",
    "    \"analysis\": <your analysis, based on the conversations>,\n",
    "    \"result\": <the person's score on {dim}, ranging from -5 to 5>\n",
    "}}\n",
    "===\n",
    "\"\"\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"HAL 9000\" #\"Willie Soke\" #\"Gaston\" #\"Naruto Uzumaki\"#\"Sasuke Uchiha\"\n",
    "\n",
    "# read result file\n",
    "path = f\"./result/{role_name}_120.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# generate prompts\n",
    "\n",
    "#prompt of winning rate evaluation\n",
    "# prompts = generate_prompt(df, role_name, big5)\n",
    "\n",
    "#prompt of multi-dimension evaluation\n",
    "prompts = generate_dimension_prompt(df, role_name, big5)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using GPT-4o-mini to do differenr evaluations using different promts above\n",
    "responses = []\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "    answer = response.choices[0].message.content\n",
    "    responses.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save evaluation results\n",
    "save_path = f'./result/{role_name}_dimension_eval.json' #f'./result/{role_name}_4o.json'\n",
    "with open(save_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(responses, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate winning rate\n",
    "def count_rank_1(data):\n",
    "    models_to_count = [\"Tuned GPT3.5\",\"GPT3.5\", \"Tuned Phi-3\", \"Phi-3\", \"Tuned Phi-3 with simple prompt\", \"Phi-3 with simple prompt\"]\n",
    "    rank_1_count = {model: 0 for model in models_to_count}\n",
    "\n",
    "    for sublist in data:\n",
    "        for entry in sublist:\n",
    "            model = entry.get(\"model\")\n",
    "            rank = entry.get(\"rank\")\n",
    "            if model in models_to_count and rank == 1:\n",
    "                rank_1_count[model] += 1\n",
    "\n",
    "    return rank_1_count\n",
    "\n",
    "#calculate avg multi-dimension scores\n",
    "def calculate_avg_dimension_scores(data):\n",
    "    models = [\"Tuned GPT3.5\", \"GPT3.5\", \"Tuned Phi-3\", \"Phi-3\", \"Tuned Phi-3 with simple prompt\", \"Phi-3 with simple prompt\"]\n",
    "    \n",
    "    scores_sums = {model: {\n",
    "            \"factual correctness score\": 0,\n",
    "            \"personality score\": 0,\n",
    "            \"values score\": 0,\n",
    "            \"avoiding hallucination score\": 0\n",
    "        } for model in models}\n",
    "    \n",
    "    counts = {model: {\n",
    "            \"factual correctness score\": 0,\n",
    "            \"personality score\": 0,\n",
    "            \"values score\": 0,\n",
    "            \"avoiding hallucination score\": 0\n",
    "        } for model in models}\n",
    "    \n",
    "    for model_list in data:\n",
    "        for model_data in model_list:\n",
    "            model_name = model_data.get(\"model\")\n",
    "            \n",
    "            for score_type in scores_sums[model_name]:\n",
    "                score = model_data.get(score_type, 0)\n",
    "                scores_sums[model_name][score_type] += score\n",
    "                counts[model_name][score_type] += 1\n",
    "    \n",
    "    average_scores = {model: {} for model in models}\n",
    "    for model_name in scores_sums:\n",
    "        for score_type in scores_sums[model_name]:\n",
    "            if counts[model_name][score_type] > 0:\n",
    "                avg_score = scores_sums[model_name][score_type] / counts[model_name][score_type]\n",
    "            else:\n",
    "                avg_score = 0\n",
    "            average_scores[model_name][score_type] = avg_score\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "def calculate_model_average_scores(data):\n",
    "    scores = {\n",
    "        \"Tuned GPT3.5\": [],\n",
    "        \"GPT3.5\": [],\n",
    "        \"Tuned Phi-3\": [],\n",
    "        \"Phi-3\": [],\n",
    "        \"Tuned Phi-3 with simple prompt\": [],\n",
    "        \"Phi-3 with simple prompt\": []\n",
    "    }\n",
    "    \n",
    "    for model_list in data:\n",
    "        for model_data in model_list:\n",
    "            model_name = model_data[\"model\"]\n",
    "            score = model_data[\"score\"]\n",
    "            if model_name in scores:\n",
    "                scores[model_name].append(score)\n",
    "    average_scores = {}\n",
    "    for model_name, score_list in scores.items():\n",
    "        if score_list:  \n",
    "            average_scores[model_name] = sum(score_list) / len(score_list)\n",
    "        else:\n",
    "            average_scores[model_name] = 0\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "#calculate questionnaire scores\n",
    "def calculate_big5_scores(data):\n",
    "    scores = {\n",
    "        \"openness\": [],\n",
    "        \"neuroticism\": [],\n",
    "        \"extraversion\": [],\n",
    "        \"agreeableness\": [],\n",
    "        \"conscientiousness\": []\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        dimension = item.get(\"dimension\")\n",
    "        result = item.get(\"result\")\n",
    "        if dimension in scores and result is not None:\n",
    "           scores[dimension].append(result)\n",
    "    \n",
    "    average_scores = {}\n",
    "    for dimension, results in scores.items():\n",
    "        if results:\n",
    "            average_scores[dimension] = sum(results) / len(results)\n",
    "        else:\n",
    "            average_scores[dimension] = None\n",
    "    \n",
    "    return average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read evaluation responses (at start is a whole string format)\n",
    "name = \"Sasuke Uchiha\"#\"HAL 9000\"#\"Gaston\" #\"Naruto Uzumaki\" #\"Sasuke Uchiha\" #\"Willie Soke\"\n",
    "file_path = f'./result/{name}_dimension_eval.json'#f'./result/{role_name}_memory_eval.json'#f'./result/{name}_gpt_big5_eval.json' #f'./result/{name}_4o.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing-extract the dict format responses\n",
    "extracted_lists = []\n",
    "for i, item in enumerate(data):\n",
    "    try:\n",
    "        if isinstance(item, str):\n",
    "            parsed_item = json.loads(item)\n",
    "        else:\n",
    "            parsed_item = item\n",
    "        extracted_lists.append(parsed_item)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"error:{i} : {item}\")\n",
    "        print(f\"error msg:{e}\")\n",
    "\n",
    "print(extracted_lists)\n",
    "#save data after processing\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(extracted_lists, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best performed questions&answers of model\n",
    "for sublist_index, sublist in enumerate(data):\n",
    "    for dict_index, dictionary in enumerate(sublist):\n",
    "        if isinstance(dictionary, dict):  # Check if the item is a dictionary\n",
    "            if dictionary.get(\"model\") == \"Tuned GPT3.5\" and dictionary.get(\"rank\") == 1:\n",
    "                print(f\"Sublist Index: {sublist_index}, Dictionary Index: {dict_index}, Dictionary: {dictionary}\")\n",
    "        else:\n",
    "            print(f\"Item at sublist index {sublist_index}, dictionary index {dict_index} is not a dictionary: {dictionary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the evaluation responses after processing\n",
    "name = \"Sasuke Uchiha\"#\"HAL 9000\"#\"Gaston\" #\"Naruto Uzumaki\" #\"Sasuke Uchiha\" #\"Willie Soke\"\n",
    "file_path = f'./result/{name}_dimension_eval.json'#f'./result/{role_name}_memory_eval.json'#f'./result/{name}_gpt_big5_eval.json' #f'./result/{name}_4o.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get winning rate\n",
    "rank_1_count = count_rank_1(data)\n",
    "    \n",
    "for model, count in rank_1_count.items():\n",
    "    print(f\"Model: {model}, Rank 1 count: {count}, Winning Rate:{count/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get multi-dimension scores\n",
    "average_scores = calculate_avg_dimension_scores(extracted_lists)\n",
    "for model, scores in average_scores.items():\n",
    "    print(f\"{model}:\")\n",
    "    for score_type, avg_score in scores.items():\n",
    "        print(f\"  {score_type}: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get questionnaire scores\n",
    "average_scores = calculate_big5_scores(extracted_lists)\n",
    "print(average_scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
